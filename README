Web Crawler README
Overview
This project consists of two Python scripts: producer.py and consumer.py. The producer.py script sends web crawling requests to a RabbitMQ queue, and the consumer.py script listens to this queue, performs the web crawling as per the requests, and records the hits of specified keywords in a text file.

Requirements
Python 3.x
RabbitMQ Server
Python Libraries: pika, requests, beautifulsoup4
Setup
Install RabbitMQ Server: Ensure RabbitMQ is installed and running. RabbitMQ Installation Guide

Install Python Libraries: Run the following command to install the required libraries:

pip install pika requests beautifulsoup4

Clone Repository: Clone or download the repository containing the producer.py and consumer.py scripts.

Running the Scripts
producer.py
Sends crawl requests to the RabbitMQ queue.
Specify the URL to crawl, the depth of crawling, and the keywords to search for.
Navigate to the directory containing producer.py.
Run the script using Python:

python producer.py

Follow the prompts to enter the URL, depth, and keywords.
consumer.py
Listens for messages from the RabbitMQ queue.
Processes each message, performs the web crawling, and records keyword hits.
Navigate to the directory containing consumer.py.
Run the script using Python:

python consumer.py

The script will continuously listen for crawl requests and process them.

Output:
The consumer.py script outputs the results to a file named keyword_hits.txt.
The file contains information about keyword occurrences in the crawled web pages.

Notes
Ensure that RabbitMQ is running and accessible before running the scripts.
The scripts should be used in compliance with the terms of service of the websites being crawled.
