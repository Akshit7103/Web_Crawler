Web Crawler Using RabbitMQ
This project demonstrates a web crawling application that uses RabbitMQ for message queuing to distribute crawling tasks across multiple threads. The project consists of two Python scripts: producer.py and consumer.py.

Requirements
Python 3.x
RabbitMQ server
Libraries specified in requirements.txt
Setup
Install the required libraries using pip:

pip install -r requirements.txt
Ensure that RabbitMQ server is running locally or update the connection parameters in both producer.py and consumer.py to connect to your RabbitMQ server.

Usage
producer.py
The producer.py script is used to send crawl requests to the RabbitMQ server.

Run producer.py:

python producer.py
Enter the URL to crawl and the depth of the crawl when prompted.

The script will send a crawl request to the RabbitMQ server.

consumer.py
The consumer.py script listens for crawl requests from the RabbitMQ server and performs the web crawling.
Run consumer.py:

python consumer.py
The script will wait for crawl requests.

When a crawl request is received, it fetches keywords from a MySQL database and initiates a multi-threaded web crawl.
Crawl results are saved to an Excel file named keyword_hits.xlsx.

Configuration
Modify the connection parameters in both producer.py and consumer.py to connect to your RabbitMQ server.
In consumer.py, update the MySQL database connection details in the get_keywords_from_db() function.
Customize the behavior of the web crawler and the way crawl results are processed in consumer.py.

Important Notes
Ensure that RabbitMQ and MySQL servers are running and properly configured before running the scripts.
