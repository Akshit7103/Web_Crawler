Web Crawler Project
Overview
This project consists of a web crawler system that utilizes a message queue for processing web crawling requests. It includes a producer script for sending crawl requests, a consumer script for processing these requests, and a MySQL database setup for storing crawl data and keywords.

Files Description
producer.py: This script sends crawl requests to a message queue. It allows users to input URLs and a crawl depth.
consumer.py: This script consumes messages from the queue, performs web crawling based on the requests, and stores the results in the database.
database.py: Contains functions for database operations such as connecting to the MySQL database and inserting words.
web_crawler.sql: SQL script for setting up the necessary tables in the MySQL database.
README.md: This file, explaining the setup and usage of the project.
Database Setup
Create a MySQL database named web_crawler.
Use the web_crawler.sql file to create the required tables.
crawl_requests: Stores the crawl requests with fields for URLs, depth, keywords, and crawl results.
(Include other tables if any, such as words or good_words based on your setup)
Installation
Ensure Python 3.x is installed.
Install necessary packages:
Copy code
pip install pymysql pika requests beautifulsoup4
Set up a RabbitMQ server or use a cloud-based service.
Usage
Starting the Producer
Run producer.py and follow the prompts to input URLs and crawl depth.
Copy code
python producer.py
Starting the Consumer
Run consumer.py to start processing crawl requests.
Copy code
python consumer.py
Configuration
Update database connection settings in db_utils.py as per your MySQL setup.

